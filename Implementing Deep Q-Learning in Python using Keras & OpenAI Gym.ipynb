{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a817c1d5",
   "metadata": {},
   "source": [
    "CartPole是一个杆子连在一个小车上，小车可以无摩擦的左右运动，杆子（倒立摆）一开始是竖直线向上的。 小车通过左右运动使得杆子不倒。 注：施加的力大小是固定的，但减小或增大的速度不是固定的，它取决于当时杆子与竖直方向的角度。 角度不同，产生的速度和位移也不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39504c85",
   "metadata": {},
   "source": [
    "![](images/cartpole_g.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501380e6",
   "metadata": {},
   "source": [
    "CartPole is one of the simplest environments in the OpenAI gym (a game simulator). As you can see in the above animation, the goal of CartPole is to balance a pole that’s connected with one joint on top of a moving cart.\n",
    "\n",
    "Instead of pixel information, there are four kinds of information given by the state (such as the angle of the pole and position of the cart). An agent can move the cart by performing a series of actions of 0 or 1, pushing the cart left or right.\n",
    "\n",
    "We will use the keras-rl2 library here which lets us implement deep Q-learning out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4518d6b0",
   "metadata": {},
   "source": [
    "Step 1: Install keras-rl2 library\n",
    "\n",
    "https://github.com/keras-rl/keras-rl/issues/371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38694f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.12.0\n",
      "argon2-cffi==20.1.0\n",
      "astunparse==1.6.3\n",
      "async-generator==1.10\n",
      "attrs==20.3.0\n",
      "backcall==0.2.0\n",
      "bleach==3.3.0\n",
      "cachetools==4.2.1\n",
      "certifi==2020.12.5\n",
      "cffi==1.14.5\n",
      "chardet==4.0.0\n",
      "cloudpickle==1.6.0\n",
      "colorama==0.4.4\n",
      "decorator==5.0.5\n",
      "defusedxml==0.7.1\n",
      "entrypoints==0.3\n",
      "flatbuffers==1.12\n",
      "future==0.18.2\n",
      "gast==0.3.3\n",
      "google-auth==1.28.0\n",
      "google-auth-oauthlib==0.4.4\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.32.0\n",
      "gym==0.18.0\n",
      "h5py==2.10.0\n",
      "idna==2.10\n",
      "ipykernel==5.5.3\n",
      "ipython==7.22.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.6.3\n",
      "jedi==0.18.0\n",
      "Jinja2==2.11.3\n",
      "jsonschema==3.2.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==6.1.12\n",
      "jupyter-console==6.4.0\n",
      "jupyter-core==4.7.1\n",
      "jupyterlab-pygments==0.1.2\n",
      "jupyterlab-widgets==1.0.0\n",
      "Keras==2.4.3\n",
      "Keras-Preprocessing==1.1.2\n",
      "keras-rl2==1.0.4\n",
      "Markdown==3.3.4\n",
      "MarkupSafe==1.1.1\n",
      "mistune==0.8.4\n",
      "nbclient==0.5.3\n",
      "nbconvert==6.0.7\n",
      "nbformat==5.1.3\n",
      "nest-asyncio==1.5.1\n",
      "notebook==6.3.0\n",
      "numpy==1.19.5\n",
      "oauthlib==3.1.0\n",
      "opt-einsum==3.3.0\n",
      "packaging==20.9\n",
      "pandocfilters==1.4.3\n",
      "parso==0.8.2\n",
      "pickleshare==0.7.5\n",
      "Pillow==7.2.0\n",
      "prometheus-client==0.10.0\n",
      "prompt-toolkit==3.0.18\n",
      "protobuf==3.15.7\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycparser==2.20\n",
      "pyglet==1.5.0\n",
      "Pygments==2.8.1\n",
      "pyparsing==2.4.7\n",
      "pyrsistent==0.17.3\n",
      "python-dateutil==2.8.1\n",
      "pywin32==300\n",
      "pywinpty==0.5.7\n",
      "PyYAML==5.4.1\n",
      "pyzmq==22.0.3\n",
      "qtconsole==5.0.3\n",
      "QtPy==1.9.0\n",
      "requests==2.25.1\n",
      "requests-oauthlib==1.3.0\n",
      "rsa==4.7.2\n",
      "scipy==1.6.2\n",
      "Send2Trash==1.5.0\n",
      "six==1.15.0\n",
      "tensorboard==2.4.1\n",
      "tensorboard-plugin-wit==1.8.0\n",
      "tensorflow==2.4.1\n",
      "tensorflow-estimator==2.4.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.9.4\n",
      "testpath==0.4.4\n",
      "tornado==6.1\n",
      "traitlets==5.0.5\n",
      "typing-extensions==3.7.4.3\n",
      "urllib3==1.26.4\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "Werkzeug==1.0.1\n",
      "widgetsnbextension==3.5.1\n",
      "wrapt==1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f91c4",
   "metadata": {},
   "source": [
    "Step 2: Install dependencies for the CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gym\n",
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35206fad",
   "metadata": {},
   "source": [
    "Step 3: Let’s get started!\n",
    "\n",
    "First, we have to import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20804aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e5cdd",
   "metadata": {},
   "source": [
    "Then, set the relevant variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5c935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5532f8c9",
   "metadata": {},
   "source": [
    "Next, we will build a very simple single hidden layer neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46525aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4309cb7",
   "metadata": {},
   "source": [
    "Now, configure and compile our agent. We will set our policy as Epsilon Greedy and our memory as Sequential Memory because we want to store the result of actions we performed and the rewards we get for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501938df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bin_he4\\.virtualenvs\\rl-ex-fndd_rbi\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9/5000: episode: 1, duration: 1.931s, episode steps:   9, steps per second:   5, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bin_he4\\.virtualenvs\\rl-ex-fndd_rbi\\lib\\site-packages\\rl\\memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "c:\\users\\bin_he4\\.virtualenvs\\rl-ex-fndd_rbi\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   28/5000: episode: 2, duration: 0.973s, episode steps:  19, steps per second:  20, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.464901, mae: 0.541042, mean_q: 0.304381\n",
      "   37/5000: episode: 3, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.418581, mae: 0.508398, mean_q: 0.330029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bin_he4\\.virtualenvs\\rl-ex-fndd_rbi\\lib\\site-packages\\rl\\memory.py:40: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   47/5000: episode: 4, duration: 0.183s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.357435, mae: 0.483128, mean_q: 0.478620\n",
      "   55/5000: episode: 5, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.343553, mae: 0.486915, mean_q: 0.557740\n",
      "   64/5000: episode: 6, duration: 0.174s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.309573, mae: 0.486496, mean_q: 0.718290\n",
      "   73/5000: episode: 7, duration: 0.157s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.269646, mae: 0.456214, mean_q: 0.817970\n",
      "   82/5000: episode: 8, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.266701, mae: 0.463512, mean_q: 0.954194\n",
      "   91/5000: episode: 9, duration: 0.164s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.278051, mae: 0.489379, mean_q: 1.078855\n",
      "  103/5000: episode: 10, duration: 0.216s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.268768, mae: 0.493939, mean_q: 1.105583\n",
      "  112/5000: episode: 11, duration: 0.165s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.334069, mae: 0.566700, mean_q: 1.244848\n",
      "  121/5000: episode: 12, duration: 0.168s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.293979, mae: 0.575496, mean_q: 1.302071\n",
      "  130/5000: episode: 13, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.289889, mae: 0.607987, mean_q: 1.407482\n",
      "  139/5000: episode: 14, duration: 0.165s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.282197, mae: 0.620030, mean_q: 1.375345\n",
      "  149/5000: episode: 15, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.325370, mae: 0.672592, mean_q: 1.543156\n",
      "  159/5000: episode: 16, duration: 0.182s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.337956, mae: 0.732782, mean_q: 1.624159\n",
      "  168/5000: episode: 17, duration: 0.149s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.387560, mae: 0.801417, mean_q: 1.623221\n",
      "  177/5000: episode: 18, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.360464, mae: 0.844028, mean_q: 1.703223\n",
      "  186/5000: episode: 19, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.305434, mae: 0.867864, mean_q: 1.859080\n",
      "  196/5000: episode: 20, duration: 0.183s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.339864, mae: 0.879859, mean_q: 1.860043\n",
      "  206/5000: episode: 21, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.352842, mae: 0.912203, mean_q: 1.959420\n",
      "  215/5000: episode: 22, duration: 0.168s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.375746, mae: 0.967079, mean_q: 2.038754\n",
      "  226/5000: episode: 23, duration: 0.198s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.380358, mae: 0.992822, mean_q: 2.101235\n",
      "  236/5000: episode: 24, duration: 0.227s, episode steps:  10, steps per second:  44, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.439832, mae: 1.094121, mean_q: 2.183654\n",
      "  245/5000: episode: 25, duration: 0.152s, episode steps:   9, steps per second:  59, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.414990, mae: 1.114285, mean_q: 2.170021\n",
      "  255/5000: episode: 26, duration: 0.168s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.417638, mae: 1.098415, mean_q: 2.202359\n",
      "  264/5000: episode: 27, duration: 0.169s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.421119, mae: 1.165939, mean_q: 2.394035\n",
      "  274/5000: episode: 28, duration: 0.177s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.433875, mae: 1.189214, mean_q: 2.416820\n",
      "  283/5000: episode: 29, duration: 0.169s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.466323, mae: 1.233038, mean_q: 2.414611\n",
      "  292/5000: episode: 30, duration: 0.192s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.397397, mae: 1.237995, mean_q: 2.450133\n",
      "  300/5000: episode: 31, duration: 0.145s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.457413, mae: 1.269203, mean_q: 2.535960\n",
      "  313/5000: episode: 32, duration: 0.226s, episode steps:  13, steps per second:  57, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.431573, mae: 1.284600, mean_q: 2.627868\n",
      "  323/5000: episode: 33, duration: 0.185s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.364238, mae: 1.304369, mean_q: 2.665099\n",
      "  333/5000: episode: 34, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.504017, mae: 1.429680, mean_q: 2.824855\n",
      "  344/5000: episode: 35, duration: 0.247s, episode steps:  11, steps per second:  44, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.385917, mae: 1.416694, mean_q: 2.857920\n",
      "  353/5000: episode: 36, duration: 0.201s, episode steps:   9, steps per second:  45, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.499646, mae: 1.448658, mean_q: 2.943338\n",
      "  362/5000: episode: 37, duration: 0.163s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.488979, mae: 1.414415, mean_q: 2.941561\n",
      "  372/5000: episode: 38, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.530333, mae: 1.515272, mean_q: 2.964302\n",
      "  380/5000: episode: 39, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.517034, mae: 1.505250, mean_q: 3.016939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  392/5000: episode: 40, duration: 0.216s, episode steps:  12, steps per second:  56, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.917 [0.000, 1.000],  loss: 0.489871, mae: 1.468363, mean_q: 3.095916\n",
      "  403/5000: episode: 41, duration: 0.199s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.567061, mae: 1.459492, mean_q: 3.217516\n",
      "  414/5000: episode: 42, duration: 0.196s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.540188, mae: 1.464930, mean_q: 3.191311\n",
      "  424/5000: episode: 43, duration: 0.166s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.506549, mae: 1.441760, mean_q: 3.208490\n",
      "  433/5000: episode: 44, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.569924, mae: 1.528108, mean_q: 3.309878\n",
      "  446/5000: episode: 45, duration: 0.252s, episode steps:  13, steps per second:  52, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.846 [0.000, 1.000],  loss: 0.423906, mae: 1.465116, mean_q: 3.383610\n",
      "  456/5000: episode: 46, duration: 0.183s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.454765, mae: 1.509668, mean_q: 3.561764\n",
      "  466/5000: episode: 47, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.501222, mae: 1.539547, mean_q: 3.629852\n",
      "  476/5000: episode: 48, duration: 0.178s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.395597, mae: 1.497793, mean_q: 3.632427\n",
      "  486/5000: episode: 49, duration: 0.171s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.507975, mae: 1.584231, mean_q: 3.755041\n",
      "  496/5000: episode: 50, duration: 0.178s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.490342, mae: 1.584566, mean_q: 3.754724\n",
      "  505/5000: episode: 51, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.480759, mae: 1.583680, mean_q: 3.806884\n",
      "  513/5000: episode: 52, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.480889, mae: 1.563766, mean_q: 3.867922\n",
      "  521/5000: episode: 53, duration: 0.149s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.583098, mae: 1.653580, mean_q: 3.829794\n",
      "  531/5000: episode: 54, duration: 0.183s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.484067, mae: 1.646356, mean_q: 3.893897\n",
      "  541/5000: episode: 55, duration: 0.183s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.373303, mae: 1.556186, mean_q: 4.056036\n",
      "  550/5000: episode: 56, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.524901, mae: 1.672139, mean_q: 4.138150\n",
      "  560/5000: episode: 57, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.493338, mae: 1.697639, mean_q: 4.108605\n",
      "  571/5000: episode: 58, duration: 0.200s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.507429, mae: 1.699031, mean_q: 4.135214\n",
      "  581/5000: episode: 59, duration: 0.183s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.484398, mae: 1.681313, mean_q: 4.221363\n",
      "  590/5000: episode: 60, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.588518, mae: 1.757698, mean_q: 4.289629\n",
      "  602/5000: episode: 61, duration: 0.225s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.560460, mae: 1.770391, mean_q: 4.330698\n",
      "  611/5000: episode: 62, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.611040, mae: 1.831134, mean_q: 4.238998\n",
      "  620/5000: episode: 63, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.407154, mae: 1.757283, mean_q: 4.386620\n",
      "  629/5000: episode: 64, duration: 0.165s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.534516, mae: 1.827301, mean_q: 4.399676\n",
      "  637/5000: episode: 65, duration: 0.151s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.421450, mae: 1.805624, mean_q: 4.451510\n",
      "  647/5000: episode: 66, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.424408, mae: 1.838914, mean_q: 4.484977\n",
      "  655/5000: episode: 67, duration: 0.167s, episode steps:   8, steps per second:  48, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.477052, mae: 1.862271, mean_q: 4.584096\n",
      "  665/5000: episode: 68, duration: 0.184s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.540533, mae: 1.972792, mean_q: 4.507043\n",
      "  674/5000: episode: 69, duration: 0.165s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.424477, mae: 1.948364, mean_q: 4.565628\n",
      "  686/5000: episode: 70, duration: 0.232s, episode steps:  12, steps per second:  52, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.445190, mae: 2.025388, mean_q: 4.663248\n",
      "  697/5000: episode: 71, duration: 0.197s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.509584, mae: 2.075418, mean_q: 4.731791\n",
      "  708/5000: episode: 72, duration: 0.199s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.490900, mae: 2.114463, mean_q: 4.745303\n",
      "  717/5000: episode: 73, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.411463, mae: 2.103764, mean_q: 4.798620\n",
      "  728/5000: episode: 74, duration: 0.200s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.442723, mae: 2.133688, mean_q: 4.920539\n",
      "  738/5000: episode: 75, duration: 0.182s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.411808, mae: 2.168644, mean_q: 4.930163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  752/5000: episode: 76, duration: 0.249s, episode steps:  14, steps per second:  56, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 0.381163, mae: 2.178760, mean_q: 4.942778\n",
      "  760/5000: episode: 77, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.311668, mae: 2.189519, mean_q: 5.067178\n",
      "  771/5000: episode: 78, duration: 0.197s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.351925, mae: 2.218431, mean_q: 5.078398\n",
      "  782/5000: episode: 79, duration: 0.218s, episode steps:  11, steps per second:  50, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.380443, mae: 2.199909, mean_q: 5.134449\n",
      "  792/5000: episode: 80, duration: 0.224s, episode steps:  10, steps per second:  45, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.318544, mae: 2.170350, mean_q: 5.068722\n",
      "  801/5000: episode: 81, duration: 0.163s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.332698, mae: 2.196236, mean_q: 5.173610\n",
      "  812/5000: episode: 82, duration: 0.193s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.340408, mae: 2.233596, mean_q: 5.161179\n",
      "  822/5000: episode: 83, duration: 0.200s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.378740, mae: 2.275822, mean_q: 5.270788\n",
      "  832/5000: episode: 84, duration: 0.207s, episode steps:  10, steps per second:  48, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.378853, mae: 2.288024, mean_q: 5.158320\n",
      "  842/5000: episode: 85, duration: 0.184s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.324523, mae: 2.251750, mean_q: 5.136116\n",
      "  852/5000: episode: 86, duration: 0.178s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.502436, mae: 2.365310, mean_q: 5.342360\n",
      "  861/5000: episode: 87, duration: 0.287s, episode steps:   9, steps per second:  31, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.295682, mae: 2.303991, mean_q: 5.345274\n",
      "  874/5000: episode: 88, duration: 0.280s, episode steps:  13, steps per second:  46, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.376570, mae: 2.339490, mean_q: 5.223941\n",
      "  883/5000: episode: 89, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.294301, mae: 2.336029, mean_q: 5.306133\n",
      "  893/5000: episode: 90, duration: 0.179s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.417672, mae: 2.368003, mean_q: 5.332644\n",
      "  902/5000: episode: 91, duration: 0.168s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.273007, mae: 2.397599, mean_q: 5.479731\n",
      "  911/5000: episode: 92, duration: 0.164s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.409022, mae: 2.426947, mean_q: 5.365728\n",
      "  921/5000: episode: 93, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.344955, mae: 2.396561, mean_q: 5.275171\n",
      "  930/5000: episode: 94, duration: 0.151s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.320296, mae: 2.410078, mean_q: 5.314076\n",
      "  941/5000: episode: 95, duration: 0.197s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.293302, mae: 2.443987, mean_q: 5.515433\n",
      "  952/5000: episode: 96, duration: 0.184s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.422548, mae: 2.464333, mean_q: 5.504014\n",
      "  960/5000: episode: 97, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.270358, mae: 2.493238, mean_q: 5.592349\n",
      "  972/5000: episode: 98, duration: 0.200s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.333774, mae: 2.505872, mean_q: 5.503930\n",
      "  982/5000: episode: 99, duration: 0.166s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.205200, mae: 2.540087, mean_q: 5.719181\n",
      "  991/5000: episode: 100, duration: 0.165s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.358023, mae: 2.589637, mean_q: 5.737866\n",
      " 1000/5000: episode: 101, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.239481, mae: 2.534449, mean_q: 5.598194\n",
      " 1011/5000: episode: 102, duration: 0.182s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.249050, mae: 2.530604, mean_q: 5.527356\n",
      " 1020/5000: episode: 103, duration: 0.151s, episode steps:   9, steps per second:  59, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.223310, mae: 2.596538, mean_q: 5.716509\n",
      " 1030/5000: episode: 104, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.314100, mae: 2.656849, mean_q: 5.789009\n",
      " 1039/5000: episode: 105, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.221674, mae: 2.664927, mean_q: 5.757581\n",
      " 1048/5000: episode: 106, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.219956, mae: 2.656446, mean_q: 5.692791\n",
      " 1057/5000: episode: 107, duration: 0.180s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.225341, mae: 2.717668, mean_q: 5.803041\n",
      " 1067/5000: episode: 108, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.206790, mae: 2.694332, mean_q: 5.817571\n",
      " 1078/5000: episode: 109, duration: 0.181s, episode steps:  11, steps per second:  61, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.209924, mae: 2.804448, mean_q: 6.003717\n",
      " 1088/5000: episode: 110, duration: 0.170s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.188263, mae: 2.801313, mean_q: 6.022538\n",
      " 1099/5000: episode: 111, duration: 0.206s, episode steps:  11, steps per second:  53, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.231556, mae: 2.779242, mean_q: 5.792916\n",
      " 1110/5000: episode: 112, duration: 0.200s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.177654, mae: 2.813575, mean_q: 5.938406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1120/5000: episode: 113, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.201341, mae: 2.729678, mean_q: 5.672774\n",
      " 1131/5000: episode: 114, duration: 0.183s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.190577, mae: 2.881839, mean_q: 6.022272\n",
      " 1142/5000: episode: 115, duration: 0.184s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.181369, mae: 2.863822, mean_q: 5.990850\n",
      " 1153/5000: episode: 116, duration: 0.201s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.199122, mae: 2.889940, mean_q: 6.004386\n",
      " 1162/5000: episode: 117, duration: 0.163s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.208632, mae: 2.886371, mean_q: 5.960707\n",
      " 1170/5000: episode: 118, duration: 0.151s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.202166, mae: 2.894365, mean_q: 5.983442\n",
      " 1178/5000: episode: 119, duration: 0.147s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.192068, mae: 2.902855, mean_q: 5.982543\n",
      " 1186/5000: episode: 120, duration: 0.132s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.191142, mae: 2.887570, mean_q: 5.942457\n",
      " 1195/5000: episode: 121, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.173944, mae: 2.944762, mean_q: 6.065621\n",
      " 1207/5000: episode: 122, duration: 0.216s, episode steps:  12, steps per second:  56, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.166454, mae: 2.912793, mean_q: 6.000910\n",
      " 1215/5000: episode: 123, duration: 0.149s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.228286, mae: 2.779072, mean_q: 5.599996\n",
      " 1226/5000: episode: 124, duration: 0.201s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.359448, mae: 2.919895, mean_q: 5.853258\n",
      " 1236/5000: episode: 125, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.184504, mae: 2.920197, mean_q: 5.927581\n",
      " 1244/5000: episode: 126, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.175106, mae: 2.983596, mean_q: 6.037354\n",
      " 1253/5000: episode: 127, duration: 0.151s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.373616, mae: 3.208710, mean_q: 6.426839\n",
      " 1264/5000: episode: 128, duration: 0.200s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.192589, mae: 3.079608, mean_q: 6.117585\n",
      " 1275/5000: episode: 129, duration: 0.202s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.191182, mae: 3.114612, mean_q: 6.207168\n",
      " 1285/5000: episode: 130, duration: 0.179s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.355365, mae: 2.999863, mean_q: 5.881499\n",
      " 1295/5000: episode: 131, duration: 0.182s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.348032, mae: 3.075113, mean_q: 6.009180\n",
      " 1307/5000: episode: 132, duration: 0.218s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.297958, mae: 3.081762, mean_q: 6.044887\n",
      " 1317/5000: episode: 133, duration: 0.176s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.149564, mae: 3.073756, mean_q: 6.083011\n",
      " 1328/5000: episode: 134, duration: 0.202s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.322201, mae: 3.110889, mean_q: 6.096573\n",
      " 1337/5000: episode: 135, duration: 0.160s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.188222, mae: 3.052871, mean_q: 5.945717\n",
      " 1352/5000: episode: 136, duration: 0.265s, episode steps:  15, steps per second:  57, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.150905, mae: 3.233426, mean_q: 6.360160\n",
      " 1362/5000: episode: 137, duration: 0.165s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.323051, mae: 3.203753, mean_q: 6.200054\n",
      " 1372/5000: episode: 138, duration: 0.168s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.227709, mae: 3.158896, mean_q: 6.064744\n",
      " 1381/5000: episode: 139, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.175535, mae: 3.144131, mean_q: 6.064629\n",
      " 1395/5000: episode: 140, duration: 0.253s, episode steps:  14, steps per second:  55, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 0.163263, mae: 3.194041, mean_q: 6.154088\n",
      " 1404/5000: episode: 141, duration: 0.161s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.206720, mae: 3.268886, mean_q: 6.264272\n",
      " 1415/5000: episode: 142, duration: 0.183s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.589453, mae: 3.419060, mean_q: 6.558918\n",
      " 1423/5000: episode: 143, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.175564, mae: 3.412854, mean_q: 6.572517\n",
      " 1432/5000: episode: 144, duration: 0.165s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.164091, mae: 3.432655, mean_q: 6.620562\n",
      " 1441/5000: episode: 145, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.183286, mae: 3.548498, mean_q: 6.851720\n",
      " 1451/5000: episode: 146, duration: 0.184s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.781729, mae: 3.638830, mean_q: 6.964717\n",
      " 1461/5000: episode: 147, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.139997, mae: 3.739691, mean_q: 7.021418\n",
      " 1470/5000: episode: 148, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.913573, mae: 3.888990, mean_q: 7.271959\n",
      " 1480/5000: episode: 149, duration: 0.182s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.738001, mae: 3.622239, mean_q: 6.865679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1491/5000: episode: 150, duration: 0.199s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 1.311106, mae: 3.752714, mean_q: 7.076500\n",
      " 1500/5000: episode: 151, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.324453, mae: 3.807926, mean_q: 7.153075\n",
      " 1509/5000: episode: 152, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.911833, mae: 3.876981, mean_q: 7.329408\n",
      " 1518/5000: episode: 153, duration: 0.165s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.850518, mae: 3.755797, mean_q: 6.982300\n",
      " 1527/5000: episode: 154, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.187707, mae: 3.835242, mean_q: 7.201455\n",
      " 1536/5000: episode: 155, duration: 0.165s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.073155, mae: 3.975873, mean_q: 7.315615\n",
      " 1547/5000: episode: 156, duration: 0.203s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 1.355655, mae: 3.922263, mean_q: 7.311893\n",
      " 1555/5000: episode: 157, duration: 0.146s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.627148, mae: 3.973351, mean_q: 7.361591\n",
      " 1565/5000: episode: 158, duration: 0.183s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.272128, mae: 4.033853, mean_q: 7.375644\n",
      " 1573/5000: episode: 159, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.142290, mae: 4.094271, mean_q: 7.499729\n",
      " 1583/5000: episode: 160, duration: 0.188s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.455158, mae: 3.889168, mean_q: 7.199522\n",
      " 1592/5000: episode: 161, duration: 0.260s, episode steps:   9, steps per second:  35, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.981339, mae: 4.031081, mean_q: 7.425504\n",
      " 1600/5000: episode: 162, duration: 0.287s, episode steps:   8, steps per second:  28, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.365206, mae: 3.851664, mean_q: 7.286901\n",
      " 1610/5000: episode: 163, duration: 0.286s, episode steps:  10, steps per second:  35, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.396504, mae: 4.093194, mean_q: 7.663444\n",
      " 1618/5000: episode: 164, duration: 0.212s, episode steps:   8, steps per second:  38, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.157453, mae: 4.082994, mean_q: 7.564576\n",
      " 1627/5000: episode: 165, duration: 0.199s, episode steps:   9, steps per second:  45, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.383887, mae: 4.270141, mean_q: 7.840019\n",
      " 1637/5000: episode: 166, duration: 0.250s, episode steps:  10, steps per second:  40, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.395837, mae: 3.964776, mean_q: 7.400193\n",
      " 1646/5000: episode: 167, duration: 0.198s, episode steps:   9, steps per second:  45, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.520648, mae: 4.208526, mean_q: 7.787094\n",
      " 1657/5000: episode: 168, duration: 0.234s, episode steps:  11, steps per second:  47, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 2.086131, mae: 4.294837, mean_q: 7.910662\n",
      " 1667/5000: episode: 169, duration: 0.241s, episode steps:  10, steps per second:  41, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 1.366945, mae: 4.100066, mean_q: 7.631161\n",
      " 1676/5000: episode: 170, duration: 0.303s, episode steps:   9, steps per second:  30, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.207007, mae: 4.359368, mean_q: 8.065322\n",
      " 1685/5000: episode: 171, duration: 0.213s, episode steps:   9, steps per second:  42, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.443471, mae: 4.385041, mean_q: 8.135118\n",
      " 1697/5000: episode: 172, duration: 0.226s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.576791, mae: 4.400242, mean_q: 8.202277\n",
      " 1708/5000: episode: 173, duration: 0.200s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.660821, mae: 4.451763, mean_q: 8.333340\n",
      " 1716/5000: episode: 174, duration: 0.145s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 1.290635, mae: 4.262041, mean_q: 7.980199\n",
      " 1724/5000: episode: 175, duration: 0.135s, episode steps:   8, steps per second:  59, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.881195, mae: 4.577136, mean_q: 8.426941\n",
      " 1734/5000: episode: 176, duration: 0.184s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 3.761137, mae: 4.684444, mean_q: 8.498071\n",
      " 1747/5000: episode: 177, duration: 0.230s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 2.987540, mae: 4.578823, mean_q: 8.320710\n",
      " 1755/5000: episode: 178, duration: 0.166s, episode steps:   8, steps per second:  48, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 3.004834, mae: 4.499648, mean_q: 8.116732\n",
      " 1763/5000: episode: 179, duration: 0.149s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 3.037055, mae: 4.663046, mean_q: 8.315630\n",
      " 1775/5000: episode: 180, duration: 0.198s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 2.432684, mae: 4.618514, mean_q: 8.242867\n",
      " 1786/5000: episode: 181, duration: 0.185s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 1.836053, mae: 4.458178, mean_q: 8.084553\n",
      " 1801/5000: episode: 182, duration: 0.263s, episode steps:  15, steps per second:  57, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.591980, mae: 4.497963, mean_q: 8.161209\n",
      " 1818/5000: episode: 183, duration: 0.302s, episode steps:  17, steps per second:  56, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 3.479055, mae: 4.643642, mean_q: 8.323512\n",
      " 1833/5000: episode: 184, duration: 0.266s, episode steps:  15, steps per second:  56, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.326426, mae: 4.682825, mean_q: 8.454215\n",
      " 1842/5000: episode: 185, duration: 0.165s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 2.816537, mae: 4.620307, mean_q: 8.256104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1868/5000: episode: 186, duration: 0.436s, episode steps:  26, steps per second:  60, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.004313, mae: 4.585264, mean_q: 8.310884\n",
      " 1882/5000: episode: 187, duration: 0.248s, episode steps:  14, steps per second:  57, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.592759, mae: 4.611460, mean_q: 8.316271\n",
      " 1903/5000: episode: 188, duration: 0.363s, episode steps:  21, steps per second:  58, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.650994, mae: 4.587945, mean_q: 8.415720\n",
      " 1913/5000: episode: 189, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 2.019940, mae: 4.749363, mean_q: 8.679503\n",
      " 1926/5000: episode: 190, duration: 0.234s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 2.176053, mae: 4.686099, mean_q: 8.551617\n",
      " 1935/5000: episode: 191, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 1.807876, mae: 4.670771, mean_q: 8.597504\n",
      " 1951/5000: episode: 192, duration: 0.282s, episode steps:  16, steps per second:  57, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.669956, mae: 4.790901, mean_q: 8.658383\n",
      " 1963/5000: episode: 193, duration: 0.198s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.997303, mae: 4.608967, mean_q: 8.409116\n",
      " 1981/5000: episode: 194, duration: 0.302s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.485560, mae: 4.874157, mean_q: 8.851846\n",
      " 1995/5000: episode: 195, duration: 0.278s, episode steps:  14, steps per second:  50, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.074306, mae: 4.722225, mean_q: 8.604579\n",
      " 2017/5000: episode: 196, duration: 0.371s, episode steps:  22, steps per second:  59, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 1.669545, mae: 4.663471, mean_q: 8.553069\n",
      " 2055/5000: episode: 197, duration: 0.649s, episode steps:  38, steps per second:  59, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.353693, mae: 4.734682, mean_q: 8.588057\n",
      " 2101/5000: episode: 198, duration: 0.818s, episode steps:  46, steps per second:  56, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.176465, mae: 4.862982, mean_q: 8.801992\n",
      " 2132/5000: episode: 199, duration: 0.571s, episode steps:  31, steps per second:  54, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 1.768271, mae: 4.814877, mean_q: 8.820739\n",
      " 2160/5000: episode: 200, duration: 0.475s, episode steps:  28, steps per second:  59, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.575279, mae: 4.798558, mean_q: 8.878749\n",
      " 2173/5000: episode: 201, duration: 0.231s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 1.262845, mae: 4.803867, mean_q: 8.924520\n",
      " 2186/5000: episode: 202, duration: 0.219s, episode steps:  13, steps per second:  59, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 1.307034, mae: 4.905286, mean_q: 9.157781\n",
      " 2196/5000: episode: 203, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 2.234244, mae: 5.009870, mean_q: 9.260500\n",
      " 2206/5000: episode: 204, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 2.208786, mae: 4.962161, mean_q: 9.172897\n",
      " 2216/5000: episode: 205, duration: 0.183s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.319858, mae: 4.912830, mean_q: 9.216213\n",
      " 2226/5000: episode: 206, duration: 0.185s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 2.508875, mae: 5.029905, mean_q: 9.165702\n",
      " 2237/5000: episode: 207, duration: 0.198s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.786726, mae: 5.014335, mean_q: 9.209779\n",
      " 2255/5000: episode: 208, duration: 0.355s, episode steps:  18, steps per second:  51, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 1.546544, mae: 4.960078, mean_q: 9.163080\n",
      " 2271/5000: episode: 209, duration: 0.279s, episode steps:  16, steps per second:  57, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 1.967344, mae: 5.030429, mean_q: 9.215563\n",
      " 2285/5000: episode: 210, duration: 0.242s, episode steps:  14, steps per second:  58, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 2.753138, mae: 5.187535, mean_q: 9.403885\n",
      " 2301/5000: episode: 211, duration: 0.274s, episode steps:  16, steps per second:  58, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.222496, mae: 5.109827, mean_q: 9.263457\n",
      " 2328/5000: episode: 212, duration: 0.463s, episode steps:  27, steps per second:  58, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.813086, mae: 5.104996, mean_q: 9.379757\n",
      " 2360/5000: episode: 213, duration: 0.545s, episode steps:  32, steps per second:  59, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.608272, mae: 5.016467, mean_q: 9.231589\n",
      " 2408/5000: episode: 214, duration: 0.847s, episode steps:  48, steps per second:  57, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.607034, mae: 5.061076, mean_q: 9.336604\n",
      " 2428/5000: episode: 215, duration: 0.351s, episode steps:  20, steps per second:  57, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.862897, mae: 5.265275, mean_q: 9.747130\n",
      " 2444/5000: episode: 216, duration: 0.284s, episode steps:  16, steps per second:  56, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.616135, mae: 5.182203, mean_q: 9.613392\n",
      " 2469/5000: episode: 217, duration: 0.435s, episode steps:  25, steps per second:  57, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 2.074179, mae: 5.229351, mean_q: 9.594186\n",
      " 2489/5000: episode: 218, duration: 0.388s, episode steps:  20, steps per second:  52, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.070904, mae: 5.260480, mean_q: 9.690995\n",
      " 2505/5000: episode: 219, duration: 0.272s, episode steps:  16, steps per second:  59, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 2.006617, mae: 5.232590, mean_q: 9.628866\n",
      " 2535/5000: episode: 220, duration: 0.525s, episode steps:  30, steps per second:  57, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.702029, mae: 5.161592, mean_q: 9.508132\n",
      " 2565/5000: episode: 221, duration: 0.517s, episode steps:  30, steps per second:  58, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.655633, mae: 5.199201, mean_q: 9.585524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2601/5000: episode: 222, duration: 0.816s, episode steps:  36, steps per second:  44, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.454843, mae: 5.228097, mean_q: 9.757804\n",
      " 2621/5000: episode: 223, duration: 0.580s, episode steps:  20, steps per second:  34, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 1.158839, mae: 5.330241, mean_q: 10.026169\n",
      " 2662/5000: episode: 224, duration: 0.823s, episode steps:  41, steps per second:  50, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.385427, mae: 5.411831, mean_q: 10.167161\n",
      " 2694/5000: episode: 225, duration: 0.563s, episode steps:  32, steps per second:  57, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.867020, mae: 5.482892, mean_q: 10.132372\n",
      " 2716/5000: episode: 226, duration: 0.370s, episode steps:  22, steps per second:  59, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.168641, mae: 5.432886, mean_q: 10.005959\n",
      " 2748/5000: episode: 227, duration: 0.547s, episode steps:  32, steps per second:  59, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.775028, mae: 5.446061, mean_q: 10.083100\n",
      " 2793/5000: episode: 228, duration: 0.766s, episode steps:  45, steps per second:  59, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 1.342855, mae: 5.465095, mean_q: 10.249290\n",
      " 2861/5000: episode: 229, duration: 1.150s, episode steps:  68, steps per second:  59, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.725721, mae: 5.514773, mean_q: 10.262044\n",
      " 2885/5000: episode: 230, duration: 0.418s, episode steps:  24, steps per second:  57, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.825513, mae: 5.736879, mean_q: 10.706070\n",
      " 2906/5000: episode: 231, duration: 0.394s, episode steps:  21, steps per second:  53, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.736375, mae: 5.689823, mean_q: 10.604443\n",
      " 2927/5000: episode: 232, duration: 0.367s, episode steps:  21, steps per second:  57, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.463423, mae: 5.673256, mean_q: 10.610321\n",
      " 2947/5000: episode: 233, duration: 0.355s, episode steps:  20, steps per second:  56, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.499967, mae: 5.687600, mean_q: 10.631471\n",
      " 2976/5000: episode: 234, duration: 0.555s, episode steps:  29, steps per second:  52, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 1.717252, mae: 5.632862, mean_q: 10.489372\n",
      " 3055/5000: episode: 235, duration: 1.721s, episode steps:  79, steps per second:  46, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 1.760565, mae: 5.817567, mean_q: 10.889761\n",
      " 3129/5000: episode: 236, duration: 1.250s, episode steps:  74, steps per second:  59, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.492236, mae: 5.915138, mean_q: 11.114170\n",
      " 3162/5000: episode: 237, duration: 0.644s, episode steps:  33, steps per second:  51, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.415071, mae: 5.977061, mean_q: 11.261267\n",
      " 3201/5000: episode: 238, duration: 0.670s, episode steps:  39, steps per second:  58, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 1.486201, mae: 6.150056, mean_q: 11.617850\n",
      " 3252/5000: episode: 239, duration: 0.864s, episode steps:  51, steps per second:  59, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.782290, mae: 6.217243, mean_q: 11.666893\n",
      " 3279/5000: episode: 240, duration: 0.464s, episode steps:  27, steps per second:  58, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.209267, mae: 6.197538, mean_q: 11.717721\n",
      " 3334/5000: episode: 241, duration: 1.048s, episode steps:  55, steps per second:  52, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 1.715026, mae: 6.271034, mean_q: 11.764181\n",
      " 3371/5000: episode: 242, duration: 0.649s, episode steps:  37, steps per second:  57, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.866734, mae: 6.307458, mean_q: 11.792874\n",
      " 3426/5000: episode: 243, duration: 0.932s, episode steps:  55, steps per second:  59, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 1.710568, mae: 6.399400, mean_q: 12.008768\n",
      " 3480/5000: episode: 244, duration: 0.900s, episode steps:  54, steps per second:  60, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.747550, mae: 6.512274, mean_q: 12.267967\n",
      " 3579/5000: episode: 245, duration: 1.683s, episode steps:  99, steps per second:  59, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 1.814710, mae: 6.630592, mean_q: 12.465066\n",
      " 3617/5000: episode: 246, duration: 0.667s, episode steps:  38, steps per second:  57, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.741162, mae: 6.579577, mean_q: 12.424556\n",
      " 3739/5000: episode: 247, duration: 2.048s, episode steps: 122, steps per second:  60, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 1.969707, mae: 6.823543, mean_q: 12.861944\n",
      " 3769/5000: episode: 248, duration: 0.536s, episode steps:  30, steps per second:  56, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.507053, mae: 7.047210, mean_q: 13.294772\n",
      " 3841/5000: episode: 249, duration: 1.364s, episode steps:  72, steps per second:  53, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.859815, mae: 7.090265, mean_q: 13.411146\n",
      " 3881/5000: episode: 250, duration: 0.782s, episode steps:  40, steps per second:  51, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.156014, mae: 7.251899, mean_q: 13.706195\n",
      " 3939/5000: episode: 251, duration: 1.113s, episode steps:  58, steps per second:  52, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.933914, mae: 7.155864, mean_q: 13.522838\n",
      " 4024/5000: episode: 252, duration: 1.547s, episode steps:  85, steps per second:  55, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 2.267062, mae: 7.303230, mean_q: 13.792486\n",
      " 4074/5000: episode: 253, duration: 1.033s, episode steps:  50, steps per second:  48, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.663469, mae: 7.389565, mean_q: 14.057734\n",
      " 4176/5000: episode: 254, duration: 1.700s, episode steps: 102, steps per second:  60, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.011408, mae: 7.598114, mean_q: 14.428362\n",
      " 4277/5000: episode: 255, duration: 1.782s, episode steps: 101, steps per second:  57, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 2.036173, mae: 7.705028, mean_q: 14.664177\n",
      " 4357/5000: episode: 256, duration: 1.450s, episode steps:  80, steps per second:  55, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.733588, mae: 7.943442, mean_q: 15.073651\n",
      " 4421/5000: episode: 257, duration: 1.082s, episode steps:  64, steps per second:  59, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.295581, mae: 8.052099, mean_q: 15.361383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4479/5000: episode: 258, duration: 0.985s, episode steps:  58, steps per second:  59, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 2.091609, mae: 8.147963, mean_q: 15.608634\n",
      " 4568/5000: episode: 259, duration: 1.581s, episode steps:  89, steps per second:  56, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.375388, mae: 8.253555, mean_q: 15.777792\n",
      " 4732/5000: episode: 260, duration: 3.861s, episode steps: 164, steps per second:  42, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 2.679690, mae: 8.570634, mean_q: 16.377707\n",
      " 4777/5000: episode: 261, duration: 1.688s, episode steps:  45, steps per second:  27, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.364439, mae: 8.684531, mean_q: 16.536690\n",
      " 4846/5000: episode: 262, duration: 1.482s, episode steps:  69, steps per second:  47, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.430896, mae: 8.786442, mean_q: 16.804522\n",
      " 4920/5000: episode: 263, duration: 1.940s, episode steps:  74, steps per second:  38, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 3.211436, mae: 8.919266, mean_q: 16.985533\n",
      "done, took 98.814 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29e790a6160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10, \n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178242cb",
   "metadata": {},
   "source": [
    "Test our reinforcement learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53a1fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2240d280d30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3fb70f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
